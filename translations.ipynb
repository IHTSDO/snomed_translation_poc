{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cf43dc-957d-4bf7-88cd-80ae317041b0",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook compares DeepL and Aya translations over a selected sub-set of SNOMED concepts.\n",
    "\n",
    "1. We run DeepL over the entire subset.\n",
    "2. We evaluate Aya with a simple prompt over the entire subset.\n",
    "3. We evaluate Aya with a richer prompt, constructed using RAG techniques over the terminology.\n",
    "4. We perform a minimal fine-tune of Aya and evaluate whether translation quality has improved.\n",
    "5. We export a grid of results for analysis in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c36229-2afa-4b82-bb1f-c6c44c31bae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True if we want to make translation evaluation case-insensitive\n",
    "ignore_case = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aab5f7-ce87-4ba0-b8c8-1657c1c34201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set to True if running locally, False if running on AWS SageMaker\n",
    "local_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77850b77-a755-400a-9bd0-c6b3a8c7bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if we want to re-use an existing fine-tuning dataset\n",
    "use_existing_sft_dataset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40366b53-0527-4e98-848c-452c0e8a21ca",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a1dd7b-dff0-4861-bf42-9c20854942ae",
   "metadata": {},
   "source": [
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930face-aac9-4b0a-98e8-530c66b3b4de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not local_run:\n",
    "    !pip install transformers\n",
    "    !pip install deepl\n",
    "    !pip install tqdm\n",
    "    !pip install evaluate\n",
    "    !pip install termcolor\n",
    "    !pip install Levenshtein\n",
    "    !pip install nltk\n",
    "    !pip install cer\n",
    "    !pip install accelerate\n",
    "    !pip install wandb\n",
    "    !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5b21c-ef32-49cf-a96b-57d98093036a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not local_run:\n",
    "    # Redirect cache so we don't fill the disk\n",
    "    import os\n",
    "    os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87bd99-b973-4435-bc22-f425447347e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer, TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from snomed_graph import *\n",
    "import getpass\n",
    "import deepl\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from termcolor import colored\n",
    "from collections import namedtuple\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "from operator import __or__\n",
    "from functools import reduce\n",
    "from ast import literal_eval\n",
    "from Levenshtein import ratio\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa931693-9d38-442f-9a31-22d7a6b4609a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter the DeepL API key here\n",
    "DEEPL_AUTH_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cc59c-57ee-4d6c-85db-9a3f7a7a7805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "PATH_TO_SERIALIZED_SNOMED_GRAPH = \"./data/snomed_graph/full_concept_graph.gml\"\n",
    "PATH_TO_TRANSLATION_SAMPLES = \"./data/prepared_translation_data/samples.csv\"\n",
    "PATH_TO_ALL_TRANSLATION_REFERENCES = \"./data/prepared_translation_data/all_translations.csv\"\n",
    "PATH_TO_DEEPL_TRANSLATION_RESULTS = \"./data/cache/deepl_results.json\"\n",
    "PATH_TO_AYA_VANILLA_TRANSLATION_RESULTS = \"./data/cache/aya_results_vanilla.json\"\n",
    "PATH_TO_AYA_ENRICHED_TRANSLATION_RESULTS = \"./data/cache/aya_results_enriched.json\"\n",
    "TRANSLATIONS_OUTPUT_PATH = \"./data/translation_outputs/translations.csv\"\n",
    "GRID_OUTPUT_PATH = \"./data/translation_outputs/grid.csv\"\n",
    "PATH_TO_SFT_DATASET = \"./data/sft_dataset\"\n",
    "ADAPTOR_PATH = \"./models/adaptors/aya_finetuned\"\n",
    "\n",
    "# The Aya checkpoint on HuggingFace\n",
    "AYA_CHECKPOINT = \"CohereForAI/aya-101\"\n",
    "\n",
    "# Reproducible experiments\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# The number of examples to use to evaluate the fine-tuning results.\n",
    "SFT_EVAL_EXAMPLES = 500\n",
    "\n",
    "# The % of selected fine-tuning examples to reserve for testing.\n",
    "SFT_TEST_PCT = 0.9\n",
    "\n",
    "# The languages to fine-tune over\n",
    "SFT_LANGS = [\"Dutch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20823393-3ee4-4ddd-9957-0a9cf900ecab",
   "metadata": {},
   "source": [
    "This is the reference data we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93502126-6915-4c0c-858b-eecf7620077a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langcodes = {\n",
    "    \"Dutch\": \"NL\",\n",
    "    \"Estonian\": \"ET\",\n",
    "    \"Korean\": \"KO\",\n",
    "    \"Swedish\": \"SV\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d80ea-49fc-456c-97ad-44a95b3f015d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hierarchies_in_use = [\n",
    "    \"substance\",\n",
    "    \"body structure\",\n",
    "    \"finding\",\n",
    "    \"disorder\",\n",
    "    \"procedure\",\n",
    "    \"morphologic abnormality\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b20036-b940-47f3-ba11-864c67f3a85d",
   "metadata": {},
   "source": [
    "Note that some \"low value\" attributes have been removed because the relationships are unlikely to yield any value to a translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d61637-bd8d-4f78-984a-99fc42b96cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "important_attributes = {\n",
    "    # 'Access (attribute)',\n",
    "    # 'After (attribute)',\n",
    "    'Associated finding (attribute)',\n",
    "    'Associated morphology (attribute)',\n",
    "    'Associated procedure (attribute)',\n",
    "    'Associated with (attribute)',\n",
    "    'Before (attribute)',\n",
    "    'Causative agent (attribute)',\n",
    "    'Characterizes (attribute)',\n",
    "    # 'Clinical course (attribute)',\n",
    "    'Component (attribute)',\n",
    "    'Direct device (attribute)',\n",
    "    'Direct morphology (attribute)',\n",
    "    'Direct site (attribute)',\n",
    "    'Direct substance (attribute)',\n",
    "    'Due to (attribute)',\n",
    "    'During (attribute)',\n",
    "    # 'Finding context (attribute)',\n",
    "    'Finding informer (attribute)',\n",
    "    'Finding method (attribute)',\n",
    "    'Finding site (attribute)',\n",
    "    'Has absorbability (attribute)',\n",
    "    'Has active ingredient (attribute)',\n",
    "    'Has basic dose form (attribute)',\n",
    "    'Has basis of strength substance (attribute)',\n",
    "    'Has coating material (attribute)',\n",
    "    'Has compositional material (attribute)',\n",
    "    'Has concentration strength denominator unit (attribute)',\n",
    "    'Has concentration strength numerator unit (attribute)',\n",
    "    'Has device intended site (attribute)',\n",
    "    'Has disposition (attribute)',\n",
    "    'Has dose form administration method (attribute)',\n",
    "    'Has dose form intended site (attribute)',\n",
    "    'Has dose form release characteristic (attribute)',\n",
    "    'Has dose form transformation (attribute)',\n",
    "    'Has filling (attribute)',\n",
    "    'Has focus (attribute)',\n",
    "    'Has ingredient qualitative strength (attribute)',\n",
    "    'Has intent (attribute)',\n",
    "    # 'Has interpretation (attribute)',\n",
    "    'Has manufactured dose form (attribute)',\n",
    "    'Has precise active ingredient (attribute)',\n",
    "    'Has presentation strength denominator unit (attribute)',\n",
    "    'Has presentation strength numerator unit (attribute)',\n",
    "    'Has realization (attribute)',\n",
    "    'Has specimen (attribute)',\n",
    "    'Has state of matter (attribute)',\n",
    "    'Has surface texture (attribute)',\n",
    "    'Has target population (attribute)',\n",
    "    'Has unit of presentation (attribute)',\n",
    "    'Indirect device (attribute)',\n",
    "    'Indirect morphology (attribute)',\n",
    "    'Inherent location (attribute)',\n",
    "    'Inheres in (attribute)',\n",
    "    'Interprets (attribute)',\n",
    "    # 'Is a (attribute)',\n",
    "    'Is modification of (attribute)',\n",
    "    'Is sterile (attribute)',\n",
    "    'Laterality (attribute)',\n",
    "    'Measurement method (attribute)',\n",
    "    'Method (attribute)',\n",
    "    'Occurrence (attribute)',\n",
    "    'Pathological process (attribute)',\n",
    "    'Plays role (attribute)',\n",
    "    'Precondition (attribute)',\n",
    "    'Priority (attribute)',\n",
    "    'Procedure context (attribute)',\n",
    "    'Procedure device (attribute)',\n",
    "    'Procedure morphology (attribute)',\n",
    "    'Procedure site (attribute)',\n",
    "    'Procedure site - Direct (attribute)',\n",
    "    'Procedure site - Indirect (attribute)',\n",
    "    'Process acts on (attribute)',\n",
    "    'Process duration (attribute)',\n",
    "    'Process extends to (attribute)',\n",
    "    'Process output (attribute)',\n",
    "    'Property (attribute)',\n",
    "    'Recipient category (attribute)',\n",
    "    'Relative to (attribute)',\n",
    "    'Relative to part of (attribute)',\n",
    "    'Revision status (attribute)',\n",
    "    'Route of administration (attribute)',\n",
    "    # 'Scale type (attribute)',\n",
    "    # 'Severity (attribute)',\n",
    "    'Specimen procedure (attribute)',\n",
    "    'Specimen source identity (attribute)',\n",
    "    'Specimen source morphology (attribute)',\n",
    "    'Specimen source topography (attribute)',\n",
    "    'Specimen substance (attribute)',\n",
    "    # 'Subject relationship context (attribute)',\n",
    "    'Surgical approach (attribute)',\n",
    "    'Technique (attribute)',\n",
    "    # 'Temporal context (attribute)',\n",
    "    # 'Temporally related to (attribute)',\n",
    "    # 'Time aspect (attribute)',\n",
    "    # 'Units (attribute)',\n",
    "    'Using access device (attribute)',\n",
    "    'Using device (attribute)',\n",
    "    'Using energy (attribute)',\n",
    "    'Using substance (attribute)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f1ac12-d370-4c13-958a-ed503cfcb007",
   "metadata": {},
   "source": [
    "# 2. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf79ad-498a-49d7-8704-ea8d5dd952c5",
   "metadata": {},
   "source": [
    "## 2.1 Load the concepts to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a6ccb-8e5a-4727-a45d-d8cb60cd4916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns are: sctid, fsn, hierarchy, language, context_tier, depth_tier, concept_length_bucket, translations\n",
    "all_df = (\n",
    "    pd.read_csv(PATH_TO_TRANSLATION_SAMPLES)\n",
    "    .set_index([\"sctid\", \"language\"])\n",
    ")\n",
    "\n",
    "all_df.reference_translations = all_df.reference_translations.apply(literal_eval)\n",
    "\n",
    "all_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7455e7-6667-4abd-b49e-4d5bd780a33f",
   "metadata": {},
   "source": [
    "## 2.2 Load the full set of reference translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e90b69-f3cb-4a04-9548-5bb26c913774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns are: sctid, fsn, hierarchy, language, context_tier, depth_tier, concept_length_bucket, translations\n",
    "ref_df = (\n",
    "    pd.read_csv(PATH_TO_ALL_TRANSLATION_REFERENCES)\n",
    "    .set_index([\"sctid\", \"language\"])\n",
    ")\n",
    "\n",
    "ref_df.reference_translations = ref_df.reference_translations.apply(lambda x: literal_eval(x) if x is not np.nan else pd.NA)\n",
    "\n",
    "ref_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6e15f-15e9-4284-a4ee-e2345a80cfe3",
   "metadata": {},
   "source": [
    "## 2.3 Load the SNOMED graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb2042-56ec-40ac-9c55-cedfa18a3c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = SnomedGraph.from_serialized(PATH_TO_SERIALIZED_SNOMED_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5d051-4705-446b-ae51-f47fb39634bf",
   "metadata": {},
   "source": [
    "# 3. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8907f3d-f1a3-4738-afa3-9179dd756ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Google BLEU\n",
    "# Max of precision, recall of all ngrams (of 1-4 tokens)\n",
    "# Higher is better\n",
    "# https://huggingface.co/spaces/evaluate-metric/google_bleu\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "\n",
    "# CharacTER\n",
    "# Roughly: min # char edits required to match pred to ref, normalized by pred len\n",
    "# Lower is better\n",
    "# https://huggingface.co/spaces/evaluate-metric/character\n",
    "character = evaluate.load(\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf8095-2b5a-4790-85fa-894daa4faa62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Matches evaluate library outputs\n",
    "def exact_match(predictions, references):\n",
    "    N = len(predictions)\n",
    "    n = 0\n",
    "    for p, r in zip(predictions, references):\n",
    "        if p in r:\n",
    "            n += 1\n",
    "    return {'exact_match': float(n)/N}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778f47f-c500-4ebf-aff1-5f283165ba70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Levenshtein Ratio\n",
    "# 1 - [Levenshtein Dist] / [Sum of lengths]\n",
    "# Higher is better\n",
    "# https://rapidfuzz.github.io/Levenshtein/levenshtein.html#ratio\n",
    "def levenshtein_ratio(predictions, references):\n",
    "    ratios = [\n",
    "        np.max([ratio(p, r) for r in refs])\n",
    "        for p, refs in zip(predictions, references)\n",
    "    ]\n",
    "    return {'levenshtein_ratio': np.mean(ratios)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef54b57-bbe4-49bb-b1db-cfc2bc47e084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_translations(row_or_df, target_column, ignore_case):\n",
    "    if isinstance(row_or_df, pd.DataFrame):\n",
    "        assert target_column in row_or_df.columns    \n",
    "        candidates = list(row_or_df.to_dict()[target_column].values())\n",
    "        references = row_or_df.reference_translations.tolist()\n",
    "    else:\n",
    "        candidates = [getattr(row_or_df, target_column)]\n",
    "        references = [row_or_df.reference_translations]\n",
    "    if ignore_case:\n",
    "        candidates = [c.lower() for c in candidates]\n",
    "        references = [[r.lower() for r in refs] for refs in references]\n",
    "    results = [\n",
    "        exact_match(predictions=candidates, references=references),\n",
    "        levenshtein_ratio(predictions=candidates, references=references),\n",
    "        google_bleu.compute(predictions=candidates, references=references),\n",
    "        character.compute(predictions=candidates, references=references),\n",
    "    ]\n",
    "    results = reduce(__or__, results, dict())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15910fa5-05da-4684-8434-6569ddc09304",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Generate baseline translations with DeepL\n",
    "\n",
    "To save costs, we cache translations.  This means that whenever we re-run the code, we only translate new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876d773-7864-4396-9418-ff7f0b9747ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = deepl.Translator(DEEPL_AUTH_KEY)\n",
    "\n",
    "def translate_with_deepl(df, G):\n",
    "    \n",
    "    with open(PATH_TO_DEEPL_TRANSLATION_RESULTS, \"r\") as f:\n",
    "        deepl_results = json.load(f)\n",
    "    \n",
    "    for it, row in enumerate(tqdm(df.itertuples(), total=df.shape[0])):\n",
    "        sctid, language = row.Index\n",
    "        langcode = langcodes[language]\n",
    "        source_concept = G.get_concept_details(sctid)\n",
    "        source_preferred_term = source_concept.fsn.replace(f\"({source_concept.hierarchy})\", \"\").strip()\n",
    "        key = str(sctid) + \"_\" + language\n",
    "        try:\n",
    "            yield deepl_results[key]\n",
    "        except KeyError:\n",
    "            deepl_result = translator.translate_text(source_preferred_term, target_lang=langcode)\n",
    "            deepl_results[key] = deepl_result.text\n",
    "            yield deepl_result.text\n",
    "        if it % 100 == 0:\n",
    "            with open(PATH_TO_DEEPL_TRANSLATION_RESULTS, \"w\") as f:\n",
    "                json.dump(deepl_results, f)\n",
    "\n",
    "    with open(PATH_TO_DEEPL_TRANSLATION_RESULTS, \"w\") as f:\n",
    "        json.dump(deepl_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd516d-1f59-4167-bd43-5f7949915ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df[\"deepl_translation\"] = list(translate_with_deepl(all_df, G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9c100-c32b-4f24-b590-5a00d75c9f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_translations(all_df, \"deepl_translation\", ignore_case=ignore_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b99278-9f6e-4ba4-b528-9e08c7a5c9d1",
   "metadata": {},
   "source": [
    "# 5. Load Aya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b8007-147c-4157-9109-0f752630daf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(AYA_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b68eb-dc10-498e-8f50-f7c66559ae04",
   "metadata": {},
   "source": [
    "N.B. T5-derivative models (like Aya) do not support FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a541c0-7c6b-49a8-b7e9-b99c88551bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if local_run:    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    aya_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        AYA_CHECKPOINT, \n",
    "        device_map=\"cuda\", \n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "else:\n",
    "    aya_model = AutoModelForSeq2SeqLM.from_pretrained(AYA_CHECKPOINT, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158228c-7532-46c2-b064-081386a5969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevents generation from being prematurely truncated.\n",
    "# (Otherwise we'll end up using the model default, which could be too small.)\n",
    "# Currently commented out because it leads to decoding errors during generation.\n",
    "# Also, most (all) responses are 20 tokens, or less.\n",
    "\n",
    "# generation_config = deepcopy(aya_model.generation_config)\n",
    "# generation_config.update(max_new_tokens = 64)\n",
    "# generation_config.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a5cab-422c-47e3-b621-f768cfffcffd",
   "metadata": {},
   "source": [
    "## 5.1 Pre and post-processing wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b4e79-e538-490f-b5c6-69315c275951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aya_postprocessor(result):\n",
    "    return (\n",
    "        result\n",
    "        .replace(tokenizer.eos_token, \"\")\n",
    "        .replace(tokenizer.pad_token, \"\")\n",
    "        .replace(\".\", \"\")\n",
    "        .strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e947485-cfe1-4db4-ba14-abd7bfa83789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_with_aya(df, prompt_col, results_filepath=None, rebuild=False, save=False):\n",
    "\n",
    "    if rebuild:\n",
    "        results = dict()\n",
    "    else:\n",
    "        with open(results_filepath, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    \n",
    "    for i, row in tqdm(enumerate(df.itertuples()), total=df.shape[0]):\n",
    "        sctid, language = row.Index\n",
    "        key = str(sctid) + \"_\" + language\n",
    "        try:\n",
    "            yield results[key]\n",
    "        except KeyError:\n",
    "            prompt = getattr(row, prompt_col)\n",
    "            input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = aya_model.generate(input, max_new_tokens=256)\n",
    "            result = tokenizer.decode(output[0])\n",
    "            result = aya_postprocessor(result)\n",
    "            results[key] = result\n",
    "            yield result\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if save:\n",
    "                with open(results_filepath, \"w\") as f:\n",
    "                    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6358090-30c6-45ca-95fa-789e361718d6",
   "metadata": {},
   "source": [
    "# 6. Evaluate Aya Translations\n",
    "\n",
    "We use our final prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a8ee3-d290-43db-9786-a84312e8206f",
   "metadata": {},
   "source": [
    "## 6.1 Prepare the search index for similar concept retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9c88a-4cac-4cad-915b-230b7103d96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_similarity_search_keys(df):\n",
    "    languages = df.index.get_level_values(1).unique()\n",
    "    keys = dict()\n",
    "    for l in languages:\n",
    "        docs = [\n",
    "            (row.Index[0], row.fsn.replace(f\"({row.hierarchy})\", \"\").strip())\n",
    "            for row in df[df.index.get_level_values(1) == l].itertuples()\n",
    "        ]\n",
    "        values, terms = list(zip(*docs))\n",
    "        vectorizer = CountVectorizer(lowercase=True, stop_words=None, ngram_range=(2,10), binary=True)\n",
    "        key_matrix = vectorizer.fit_transform(terms)\n",
    "        keys[l] = (vectorizer, key_matrix, values)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dc832-35d7-450c-8227-ec1fa4136ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = generate_similarity_search_keys(ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90526a-c085-41ed-9ead-c599d90daf19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar(keys, row, G, ref_df, k=5, min_score=2, remove_descendants=True, remove_parents=True):\n",
    "    sctid = row.Index[0]\n",
    "    language = row.Index[1]\n",
    "    vectorizer, key_matrix, values = keys[language]\n",
    "    term = row.fsn.replace(f\"({row.hierarchy})\", \"\").strip()\n",
    "    query = vectorizer.transform([term])\n",
    "    search = key_matrix.dot(query.T).A.ravel()\n",
    "    top_k = np.argsort(-search)[0:k+1]\n",
    "    scores = search[top_k]\n",
    "    top_k = top_k[scores >= min_score]\n",
    "    results = set(np.array(values)[top_k])\n",
    "    results = {r for r in results if ref_df.loc[r, language].has_translation}\n",
    "    if remove_descendants:\n",
    "        descendants = {c.sctid for c in G.get_descendants(sctid)}\n",
    "    else:\n",
    "        descendants = set()\n",
    "    if remove_parents:\n",
    "        parents = {c.sctid for c in G.get_parents(sctid)}\n",
    "    else:\n",
    "        parents = set()        \n",
    "    results = results - {sctid} - descendants - parents\n",
    "    if results != set():\n",
    "        concepts = [G.get_concept_details(r) for r in results]\n",
    "        preferred_terms = [c.fsn.replace(f\"({c.hierarchy})\", \"\").strip() for c in concepts]\n",
    "        reference_translations = [ref_df.loc[r, language].reference_translations[0] for r in results]\n",
    "        return list(zip(preferred_terms, reference_translations))\n",
    "    else:\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798159e-2369-405a-b238-879e66949bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Match function signature for the other prompt compiling functions\n",
    "find_similar_ = partial(find_similar, keys=keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ed438-a482-4c85-8218-663682fa83be",
   "metadata": {},
   "source": [
    "## 6.2 Minimal prompt\n",
    "\n",
    "Provide Aya with nothing further than a straightforward translation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eaa7ea-ed1c-43c1-9642-126ea7681208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_minimal_aya_prompt(row, G):\n",
    "    sctid, language = row.Index\n",
    "    concept = G.get_full_concept(sctid)\n",
    "    preferred_term = concept.fsn.replace(f\"({concept.hierarchy})\", \"\").strip()\n",
    "    prompt = f'Translate the following clinical concept into {language}: \"{preferred_term}\". '\n",
    "    return prompt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d153da2-b3e9-4b67-b9a8-833d2ffd1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test the prompt-compilation here\n",
    "print(prepare_minimal_aya_prompt(next(all_df.sample(1).itertuples()), G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682accc-326d-463d-8457-3150799455d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"minimal_aya_prompt\"] = [prepare_minimal_aya_prompt(row, G) for row in tqdm(all_df.itertuples(), total=all_df.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaaf4e2-279b-4b27-8ca4-0ed841b0806b",
   "metadata": {},
   "source": [
    "## 6.3 Combined Prompt-compilation\n",
    "\n",
    "This prompt uses RAG to improve the translation.  If no suitable exemplars are retrieved, then a \"default exemplar\" is used to steer behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48008c0-314c-45a4-823b-b8031ae58399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_default_exemplar(language):\n",
    "    if language == \"Swedish\":\n",
    "        return f'Translate the following clinical concept into Swedish: \"Pain disorder with psychological factor\". smärtsyndrom med psykologisk faktor.'\n",
    "    elif language == \"Estonian\":\n",
    "        return f'Translate the following clinical concept into Estonian: \"Osseous choristoma\". Luuline koristoom.'\n",
    "    elif language == \"Korean\":\n",
    "        return f'Translate the following clinical concept into Korean: \"Endoscopic excision of lesion of esophagus\". 식도 병변 내시경 절제.'\n",
    "    elif language == \"Dutch\":\n",
    "        return f'Translate the following clinical concept into Dutch: \"Open repair of lumbar hernia using biological mesh\".  open hernioplastiek van hernia lumbalis met biologisch matje.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2dbfa-3bac-4efc-b7de-8967e9ff7dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_rag_aya_prompt(row, G, ref_df):\n",
    "    sctid, language = row.Index\n",
    "    concept = G.get_full_concept(sctid)\n",
    "    preferred_term = concept.fsn.replace(f\"({concept.hierarchy})\", \"\").strip()\n",
    "    parent_concepts = [\n",
    "        G.get_full_concept(p.sctid) for p in concept.parents\n",
    "    ]\n",
    "    parent_data = [\n",
    "        (\n",
    "            c.fsn.replace(f\"({c.hierarchy})\", \"\").strip(),\n",
    "            ref_df.loc[(c.sctid, language)].reference_translations[0],\n",
    "        )\n",
    "        for c in parent_concepts\n",
    "        if (c.sctid, language) in ref_df.index\n",
    "        and ref_df.loc[(c.sctid, language)].reference_translations is not pd.NA\n",
    "    ] \n",
    "    related_concepts = [\n",
    "        G.get_full_concept(r.tgt.sctid)\n",
    "        for g in concept.inferred_relationship_groups\n",
    "        for r in g.relationships\n",
    "        if r.type in important_attributes\n",
    "    ]    \n",
    "    relationship_data = [\n",
    "        (\n",
    "            c.fsn.replace(f\"({c.hierarchy})\", \"\").strip(),\n",
    "            ref_df.loc[(c.sctid, language)].reference_translations[0],\n",
    "        )\n",
    "        for c in related_concepts\n",
    "        if (c.sctid, language) in ref_df.index\n",
    "        and ref_df.loc[(c.sctid, language)].reference_translations is not pd.NA\n",
    "    ]\n",
    "    similarity_data = find_similar_(row=row, G=G, ref_df=ref_df)\n",
    "    default_exemplar = [generate_default_exemplar(language)]\n",
    "    exemplars = [\n",
    "        f'Translate the following clinical concept into {language}: \"{pt}\". {rt}.'\n",
    "        for pt, rt in chain(parent_data, relationship_data, similarity_data)\n",
    "    ]\n",
    "    if exemplars == []:\n",
    "        exemplars = default_exemplar\n",
    "    prompt_fragments = list(set(exemplars))\n",
    "    prompt = '\\n'.join(exemplars)\n",
    "    prompt += f'\\nTranslate the following clinical concept into {language}: \"{preferred_term}\". '    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2123c7-df8f-4e70-8ac6-f227c1afea7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can test the prompt-compilation here\n",
    "print(prepare_rag_aya_prompt(next(all_df.sample(1).itertuples()), G, ref_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e3175-19d4-4f8d-a9e4-a7f196ee8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"rag_aya_prompt\"] = [prepare_rag_aya_prompt(row, G, ref_df) for row in tqdm(all_df.itertuples(), total=all_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd254933-86de-42ea-a658-4525196f5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint the work\n",
    "all_df.to_csv(TRANSLATIONS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d42a1a-093b-49dc-aaac-c9f466811cce",
   "metadata": {},
   "source": [
    "## 6.3 Run the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c4a2c-26cd-404f-a9ec-a4c056295118",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df[\"rag_aya_translation\"] = list(translate_with_aya(\n",
    "    all_df, \"rag_aya_prompt\", None, rebuild=True, save=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd0b9a-5345-4a76-b18e-c6db38940f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_translations(all_df, \"rag_aya_translation\", ignore_case=ignore_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5121206-c955-4ad7-8285-6c010a839f1f",
   "metadata": {},
   "source": [
    "## 7. Score the translations at row-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f7083-d205-4b85-be0a-48914fe6094f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df[\"rag_aya_translation_scores\"] = all_df.apply(\n",
    "    lambda row: evaluate_translations(row, \"rag_aya_translation\", ignore_case=ignore_case), \n",
    "    axis=\"columns\"\n",
    ")\n",
    "tmp_df = all_df.rag_aya_translation_scores.apply(pd.Series)\n",
    "tmp_df.columns = [f\"aya_{c}\" for c in tmp_df.columns]\n",
    "all_df = all_df.drop(\"rag_aya_translation_scores\", axis=\"columns\").join(tmp_df)\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb4caf-efdd-439a-8ad7-c9fe919b1d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df[\"deepl_translation_scores\"] = all_df.apply(\n",
    "    lambda row: evaluate_translations(row, \"deepl_translation\", ignore_case=ignore_case), \n",
    "    axis=\"columns\"\n",
    ")\n",
    "tmp_df = all_df.deepl_translation_scores.apply(pd.Series)\n",
    "tmp_df.columns = [f\"deepl_{c}\" for c in tmp_df.columns]\n",
    "all_df = all_df.drop(\"deepl_translation_scores\", axis=\"columns\").join(tmp_df)\n",
    "del tmp_df\n",
    "all_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1c4f8-6b72-4869-ae46-7a7e011707ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoint the work\n",
    "all_df.to_csv(TRANSLATIONS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c9955-c731-4c10-83b9-19b0ae5d757a",
   "metadata": {},
   "source": [
    "# 8. Supervised Fine-tuning\n",
    "\n",
    "T5 uses a relative attention mechanism so we don't need to truncate our sequences.  That said, $seqlen \\propto gpumem^2$ so we need to adjust our batch sizes and/or truncate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32be81-3c85-4d44-b34e-5beb75ec4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_existing_sft_dataset:\n",
    "    ft_data = (\n",
    "        Dataset\n",
    "        .from_pandas(\n",
    "            all_df\n",
    "            [all_df.index.get_level_values(1).isin(SFT_LANGS)]\n",
    "            .reset_index()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def ft_preprocess(example):\n",
    "        model_inputs = tokenizer(\n",
    "            example[\"rag_aya_prompt\"], \n",
    "            text_target=example[\"reference_translations\"][0],\n",
    "        )\n",
    "        return model_inputs\n",
    "    \n",
    "    ft_data = ft_data.map(ft_preprocess)\n",
    "    ft_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0b873-e087-412e-8e64-42a7db8d1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data = ft_data.train_test_split(SFT_TEST_PCT)\n",
    "ft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef245471-1e00-4a7b-aff1-2cf3b0c41ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data.save_to_disk(PATH_TO_SFT_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626363f-da58-431b-b846-a72b8be1f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebccd8-9f76-4492-bd71-6e67aec81baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"snomed_translation_poc\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1bd6e-803d-47ed-b160-6fa72586ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_run:\n",
    "    aya_model = prepare_model_for_kbit_training(aya_model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "aya_model = get_peft_model(aya_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29704ec9-bc84-4fb4-a162-14875a4cd36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix a bug in Accelerate's device map construction when running on cuda:0\n",
    "if local_run:\n",
    "    aya_model.hf_device_map[''] = aya_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717de94-56dc-43ff-8202-04bccb2075d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=aya_model, \n",
    "    padding=\"longest\", \n",
    "    label_pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510639c0-cdf3-4dda-943f-168ce27eea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(aya_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acc063-abc1-4b28-b07f-b84fd38df162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs):\n",
    "    aya_translations, reference_translations = outputs\n",
    "    if isinstance(aya_translations, tuple):\n",
    "        aya_translations = aya_translations[0]\n",
    "    decoded_aya_translations = tokenizer.batch_decode(aya_translations, skip_special_tokens=True)\n",
    "    reference_translations = np.where(reference_translations != -100, reference_translations, tokenizer.pad_token_id)\n",
    "    decoded_reference_translations = tokenizer.batch_decode(reference_translations, skip_special_tokens=True)\n",
    "    levenshtein_result = levenshtein_ratio(predictions=decoded_aya_translations, references=decoded_reference_translations)\n",
    "    exact_result = exact_match(predictions=decoded_aya_translations, references=decoded_reference_translations)\n",
    "    return {**levenshtein_result, **exact_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a28622-934e-413f-ae6d-7f45b273d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that we can remove unwanted columns before training\n",
    "sft_remove_cols = [\n",
    "    k for k in ft_data[\"train\"].features.keys() \n",
    "    if not k in ['input_ids', 'attention_mask', 'labels']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad1e45-6e9a-4b48-80a6-2b09be657af6",
   "metadata": {},
   "source": [
    "N.B. tune the batch size to the available resources and throughput characteristics.\n",
    "\n",
    "1 works best for fine-tuning with a LoRA adaptor on a 4090 RTX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b5c26-dcbe-4a52-a15b-d746c15f09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = ft_data[\"train\"].remove_columns(sft_remove_cols)\n",
    "\n",
    "eval_data = (\n",
    "    ft_data[\"test\"]\n",
    "    .remove_columns(sft_remove_cols)\n",
    "    .shuffle(seed=RANDOM_SEED)\n",
    "    .select(range(SFT_EVAL_EXAMPLES))\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    report_to=\"wandb\",\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # Generation config code currently commented out\n",
    "    # generation_config=generation_config,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=aya_model,\n",
    "    args=training_args,\n",
    "    train_dataset=trn_data,\n",
    "    eval_dataset=eval_data,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de9945-e1e9-4ec5-82e7-e9d13992cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0d5cc-626e-4448-93b1-2cb1c4d648ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves adaptory only\n",
    "aya_model.save_pretrained(ADAPTOR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721925f-1595-4771-b0b3-22d5689b0f2b",
   "metadata": {},
   "source": [
    "# 9. Finetuning evaluation\n",
    "\n",
    "1. Filter to some examples that were not used for fine-tuning.\n",
    "2. Compare fine-tuned and vanilla Aya (with full prompting) across the subset.\n",
    "3. Add the SFT results to the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fc545-7010-4559-b39b-2c11596b2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_idx = list(zip(ft_data[\"test\"][\"sctid\"], ft_data[\"test\"][\"language\"]))\n",
    "sft_eval_df = all_df.loc[eval_idx].sample(SFT_EVAL_EXAMPLES)\n",
    "sft_eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6b521-2fdf-4728-bbed-ac9bd5a356b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sft_eval_df[\"sft_rag_aya_translation\"] = list(translate_with_aya(\n",
    "    sft_eval_df, \"rag_aya_prompt\", None, rebuild=True, save=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09638e-ef53-4838-bb06-a2abcef5a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_translations(sft_eval_df, \"rag_aya_translation\", ignore_case=ignore_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f8600-aa00-41cb-ad44-60d8acf6661c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_translations(sft_eval_df, \"sft_rag_aya_translation\", ignore_case=ignore_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b16e8-05b8-46fb-8816-0c53df0df3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_eval_df[\"sft_rag_aya_translation_scores\"] = sft_eval_df.apply(\n",
    "    lambda row: evaluate_translations(row, \"sft_rag_aya_translation\", ignore_case=ignore_case), \n",
    "    axis=\"columns\"\n",
    ")\n",
    "tmp_df = sft_eval_df.sft_rag_aya_translation_scores.apply(pd.Series)\n",
    "tmp_df.columns = [f\"sft_aya_{c}\" for c in tmp_df.columns]\n",
    "sft_eval_df = sft_eval_df.drop(\"sft_rag_aya_translation_scores\", axis=\"columns\").join(tmp_df)\n",
    "del tmp_df\n",
    "sft_eval_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4586592-dfd2-4201-b207-db61367b3800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the final subset of translations\n",
    "all_df = (\n",
    "    sft_eval_df\n",
    "    [['sft_rag_aya_translation', 'sft_aya_exact_match', 'sft_aya_levenshtein_ratio', 'sft_aya_google_bleu', 'sft_aya_cer_score']]\n",
    "    .join(all_df, how=\"right\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c72b6-d7ad-492d-9405-36cd3b4f95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint the work\n",
    "all_df.to_csv(TRANSLATIONS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a40e17-a184-4470-b4a2-f07de4d7fb1b",
   "metadata": {},
   "source": [
    "# 10. Final Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfc2a7-01ba-4b04-8579-b08e802a7087",
   "metadata": {},
   "source": [
    "We can reload the translations, if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d835d-e5c8-4c66-8ea2-1662bd63b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.read_csv(TRANSLATIONS_OUTPUT_PATH).set_index([\"sctid\", \"language\"])\n",
    "# all_df.reference_translations = all_df.reference_translations.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1466e-dffa-4dca-8f73-77ddc0327a52",
   "metadata": {},
   "source": [
    "First, aggregate the translation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148a312-1ab0-4948-ad82-37e133c5ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_grid_df = (\n",
    "    all_df\n",
    "    .reset_index()\n",
    "    .groupby([\"hierarchy\", \"depth_tier\", \"context_tier\", \"similarity_tier\", \"concept_length_bucket\", \"language\"])\n",
    "    .agg(\n",
    "        num_translations_tested=(\"fsn\", \"size\"),\n",
    "        aya_exact_match=(\"aya_exact_match\", \"mean\"),\n",
    "        aya_levenshtein_ratio=(\"aya_levenshtein_ratio\", \"mean\"),\n",
    "        aya_google_bleu=(\"aya_google_bleu\", \"mean\"),\n",
    "        aya_cer_score=(\"aya_cer_score\", \"mean\"),\n",
    "        deepl_exact_match=(\"deepl_exact_match\", \"mean\"),\n",
    "        deepl_levenshtein_ratio=(\"deepl_levenshtein_ratio\", \"mean\"),\n",
    "        deepl_google_bleu=(\"deepl_google_bleu\", \"mean\"),\n",
    "        deepl_cer_score=(\"deepl_cer_score\", \"mean\"),        \n",
    "        sft_aya_exact_match=(\"aya_exact_match\", \"mean\"),\n",
    "        sft_aya_levenshtein_ratio=(\"aya_levenshtein_ratio\", \"mean\"),       \n",
    "        sft_aya_google_bleu=(\"sft_aya_google_bleu\", \"mean\"),\n",
    "        sft_aya_cer_score=(\"sft_aya_cer_score\", \"mean\"),          \n",
    "    )\n",
    ")\n",
    "trans_grid_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ac70f-dd3f-42c7-b2e3-6ba4ec7b4355",
   "metadata": {},
   "source": [
    "Then, aggregate the reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518e80c-aaac-4af6-a9ad-8c5c0649381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_grid_df = (\n",
    "    ref_df\n",
    "    [ref_df.hierarchy.isin(hierarchies_in_use)]\n",
    "    .reset_index()\n",
    "    .groupby([\"hierarchy\", \"depth_tier\", \"context_tier\", \"similarity_tier\", \"concept_length_bucket\", \"language\"])\n",
    "    .agg(\n",
    "        num_concepts_in_terminology=(\"fsn\", \"size\"),\n",
    "        num_translated_concepts_in_refset=(\"has_translation\", \"sum\")\n",
    "    )\n",
    ")\n",
    "ref_grid_df[\"num_untranslated_concepts\"] = ref_grid_df.num_concepts_in_terminology - ref_grid_df.num_translated_concepts_in_refset\n",
    "ref_grid_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf72c31-1b72-43f5-9e49-343c378709b3",
   "metadata": {},
   "source": [
    "Join the two, and we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db15bdc-b12b-4bc5-88fb-809bbceebb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = ref_grid_df.join(trans_grid_df, how=\"outer\")\n",
    "grid_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113e498-2ff7-4158-a923-7914c5a4258c",
   "metadata": {},
   "source": [
    "We will be missing some values for the targets where we had no translations to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c21c2-d228-4a0a-9c30-4ec5201bdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.apply(pd.isna).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733c735-ec38-495a-846c-e2f18ceafe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df.num_translated_concepts_in_refset > 0].apply(pd.isna).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213acae-d4a4-4106-b7e6-96b2bad0f1ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can estimate the missing values with a flexible regressor.  \n",
    "\n",
    "We won't estimate for the fine-tuning examples since we only performed that for a subset of values anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cdcd9-a728-41a8-b24d-cda9e5ac7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and target columns\n",
    "features = [\"hierarchy\", \"depth_tier\", \"context_tier\", \"similarity_tier\", \"concept_length_bucket\", \"language\"]\n",
    "targets = [\"aya_exact_match\", \"aya_levenshtein_ratio\", \"aya_google_bleu\", \"aya_cer_score\", \"deepl_exact_match\", \"deepl_levenshtein_ratio\", \"deepl_google_bleu\", \"deepl_cer_score\"]\n",
    "\n",
    "# Preprocess the categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), features)\n",
    "    ])\n",
    "\n",
    "def train_model(df, target):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestRegressor(n_estimators=200))\n",
    "    ])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)    \n",
    "    print(f'Mean Squared Error for {target}: {mse}')    \n",
    "    return model\n",
    "\n",
    "# Train models for each target variable\n",
    "models = {}\n",
    "\n",
    "for target in targets:\n",
    "    models[target] = train_model(\n",
    "        grid_df[grid_df.num_translated_concepts_in_refset > 0].reset_index(), \n",
    "        target\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27e782-47c6-47bb-b0de-66d55fe01f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(row):\n",
    "    if row.num_translated_concepts_in_refset == 0:\n",
    "        for t in targets:\n",
    "            X = pd.DataFrame([{f: getattr(row, f) for f in features}])\n",
    "            y_hat = models[t].predict(X)[0]\n",
    "            setattr(row, t, y_hat)\n",
    "    return row\n",
    "    \n",
    "interpolated_grid_df = grid_df.reset_index().apply(interpolate, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2ebb7-1359-43cf-bfd3-074fd85e7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_grid_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575e234-d532-4ea3-99de-9275eb7a776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_grid_df.to_csv(GRID_OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
